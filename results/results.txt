
standardizing inputs yields 8800 mse after 1st epoch, 
but not doing it yields 3700 mse - prob bug in standardizing
64 batch size => 100 sec / epoch

fixed standardizer and it was still bad, upped lr to 0.005 and it's better

unet 101k train size: 
trainMse: 2323, valMse:2252

unet with 2xconv making images too green,
starts coloring faces properly after ~24 epochs
val loss stops decreasing after ~24 epochs

stopped working with he_normal initializer, works with glorot_uniform

loss      | channels | size   | sec/epoch | startLearn | stopLearn | lr
customLoss mul=2,     big    = 160 
customLoss mul=2,     small    = 116          5                       0.001 with 32 batch size it learns hue after 1st epoch!
                                                                          0.0005 lr = ok but too pink
MSE        mul=2,     big    = 160 
MSE        mul=1,     big    = 93           1                         0.001, saved
MSE        mul=1,     small  = 67  
MSE        mul=2,     small  = 108          2                7
customLoss mul=2,     big    = 200          5                         0.0005 

customLoss mul=1      big      97            3                           0.001 (100xhue #3) - too much same color


experimented with target normalization, leaky relu, sigmoid... didn't fare well
