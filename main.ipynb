{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showHsv(img):    \n",
    "    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 1600, 3)\n",
      "image with its value untouched, but saturation and hue randomized:\n"
     ]
    }
   ],
   "source": [
    "img_in = cv2.imread('pic.jpg')\n",
    "# maybe should switch to LAB colorspace?\n",
    "hsv = cv2.cvtColor(img_in, cv2.COLOR_BGR2HSV)\n",
    "print(hsv.shape)\n",
    "\n",
    "h,s,v = cv2.split(hsv)\n",
    "#print('s',s.shape)\n",
    "s = (np.random.random(s.shape) * 255).astype('uint8')\n",
    "h = (np.random.random(s.shape) * 255).astype('uint8')\n",
    "#s = np.maximum(0, v)\n",
    "#print(s)\n",
    "hsv = cv2.merge([h,s,v])\n",
    "print('image with its value untouched, but saturation and hue randomized:')\n",
    "#showHsv(hsv)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    break\n",
    "    pic = hsv[:,:,i]\n",
    "    plt.imshow(pic, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "(10000,) (10000,)\n",
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "X = np.arange(n).reshape(n,1)\n",
    "X = np.hstack((np.ones(n).reshape(n,1), X))\n",
    "print(X.shape)\n",
    "y = np.random.randn(n)*50 + X[:, 1]\n",
    "#y = y.reshape(n,1)\n",
    "print(y.shape, X[:,1].shape)\n",
    "#print(y)\n",
    "#plt.plot(X[:,1], y)\n",
    "yval = y\n",
    "X = np.hstack((X, yval[:,None]))\n",
    "# labels: 1 if above line, 0  below\n",
    "y = y > X[:,1]\n",
    "y = y.reshape(n,1)\n",
    "y = np.hstack((y, ~y )).astype('uint8')\n",
    "print(y.shape)\n",
    "colors = ['red' if i  else 'green' for i in y[:,0]]\n",
    "#plt.scatter(X[:,1], yval, color=colors)\n",
    "#plt.show()\n",
    "#print(y)\n",
    "#print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestModel(layers):\n",
    "    model = Sequential()\n",
    "    #print(X.shape[1])\n",
    "    model.add(Dense(layers[0], input_dim=X.shape[1], kernel_regularizer=l2(0.001)) ) \n",
    "    model.add(Activation('relu'))\n",
    "    for l in layers[1:]:\n",
    "        model.add(Dense(l, kernel_regularizer=l2(0.001)))\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImgHsv(path):\n",
    "    img_in = cv2.imread(path)\n",
    "    hsv = cv2.cvtColor(img_in, cv2.COLOR_BGR2HSV)\n",
    "    h,s,v = cv2.split(hsv)\n",
    "    X = v\n",
    "    Y = np.dstack((h,s))\n",
    "    #print(\"loadX.shape\", X.shape, 'loadY.shape', Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "# celebA dataset from https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg\n",
    "# 202,599 images of size 178x218\n",
    "\n",
    "# data generator for feeding into memory parts of dataset\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'data generator for feeding into memory parts of dataset'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(178,218), in_channels=1,\n",
    "                 out_channels=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        # target 2 channels\n",
    "        Y = np.empty((self.batch_size, *self.dim, self.out_channels))\n",
    "        print(X.shape, Y.shape)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i], Y[i] = loadImgHsv('data/img_celeba/' + ID)\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000001.jpg', '000002.jpg', '000003.jpg', '000004.jpg', '000005.jpg', '000006.jpg', '000007.jpg', '000008.jpg', '000009.jpg', '000010.jpg'] 20259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20256"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'dim': (218, 178),\n",
    "          'batch_size': 32,\n",
    "          'in_channels': 1,\n",
    "          'out_channels': 2,\n",
    "          'shuffle': True}\n",
    "\n",
    "train_IDs = os.listdir('data/img_celeba/')\n",
    "splitAt = int(len(train_IDs)*0.1)\n",
    "train_IDs = train_IDs[:splitAt]\n",
    "print(train_IDs[:10], len(train_IDs))\n",
    "\n",
    "training_generator = DataGenerator(train_IDs, **params)\n",
    "633*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-tensorboard-17-03-2019-10-56-17-layers-5x12x5\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.1245 - acc: 0.9625 - val_loss: 0.2851 - val_acc: 0.8965\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.1904 - acc: 0.9518 - val_loss: 0.0405 - val_acc: 0.9962\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.1421 - acc: 0.9567 - val_loss: 0.0860 - val_acc: 0.9635\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.1966 - acc: 0.9441 - val_loss: 0.2634 - val_acc: 0.8998\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4558 - acc: 0.9181 - val_loss: 0.1130 - val_acc: 0.9485\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.2290 - acc: 0.9447 - val_loss: 4.8313 - val_acc: 0.5962\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.2181 - acc: 0.9427 - val_loss: 0.0413 - val_acc: 0.9990\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.1265 - acc: 0.9587 - val_loss: 0.2778 - val_acc: 0.8960\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4162 - acc: 0.9145 - val_loss: 0.3331 - val_acc: 0.8868\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.1354 - acc: 0.9592 - val_loss: 0.0426 - val_acc: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1490cc18f98>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currTime = time.strftime(\"%d-%m-%Y-%H-%M-%S\", time.localtime())\n",
    "layers = [5,12,5]\n",
    "NAME = f\"test-tensorboard-{ currTime }-layers-\" + \"x\".join(str(l) for l in layers)\n",
    "print(NAME)\n",
    "\n",
    "model = createTestModel(layers)\n",
    "tensorboard = TensorBoard(log_dir=f'logs/{NAME}')\n",
    "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 35us/step\n",
      "loss:0.12557350927591324\n",
      "accuracy:0.951\n",
      "num incorrect 48\n",
      "\n",
      "wrong pred: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_classes(X).reshape((-1,))\n",
    "pred = 1 - pred\n",
    "#print(pred)\n",
    "evaluation = model.evaluate(X, y)\n",
    "print(f'loss:{evaluation[0]}\\naccuracy:{evaluation[1]}')\n",
    "labels = y[:,0]\n",
    "#print(labels.shape, labels)\n",
    "incorrects = np.nonzero(np.not_equal(pred, labels) )\n",
    "print('num incorrect', len(incorrects[0]))\n",
    "#print('incorr:',incorrects)\n",
    "print('\\nwrong pred:', pred[incorrects])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
